{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import numpy as np \n",
    "import json\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {}\n",
    "name_to_id = {}\n",
    "with open('/home/arda/thesis/eomt/output/class_names.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        id, name = line.strip().split('\\t')\n",
    "        id_to_name[int(id)] = name\n",
    "        name_to_id[name] = int(id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/arda/thesis/eomt/output/instance_counts.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "    \n",
    "metric_results = results['pq']\n",
    "instance_counts = results['instance_counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = {k.replace('_pq', ''):v for k,v in metric_results.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [run for run in metric_results.keys() if '_pred_ADE_DINO_WEIGHTS' in run]\n",
    "\n",
    "total_instance_count = 0\n",
    "per_class_metrics_overall = {}\n",
    "total_pq = 0\n",
    "\n",
    "for experiment in experiments:\n",
    "    for class_id, metrics in metric_results[experiment]['per_class_pq'].items():\n",
    "        print(experiment)\n",
    "\n",
    "        instance_count = instance_counts[experiment].get(class_id, 1)\n",
    "        if class_id not in per_class_metrics_overall:\n",
    "            per_class_metrics_overall[class_id] = [0,0]\n",
    "        per_class_metrics_overall[class_id][0] += metrics * instance_count\n",
    "        total_pq += metrics * instance_count\n",
    "        per_class_metrics_overall[class_id][1] += instance_count\n",
    "        total_instance_count += instance_count\n",
    "\n",
    "for class_id, metrics in per_class_metrics_overall.items():\n",
    "    if metrics[1] > 0:\n",
    "        per_class_metrics_overall[class_id] = metrics[0] / metrics[1]\n",
    "    else:\n",
    "        per_class_metrics_overall[class_id] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(per_class_metrics_overall.values())/len(per_class_metrics_overall.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metric_results = {}\n",
    "for name, metrics in metric_results.items():\n",
    "        \n",
    "    metrics['overall_pq'] = float(np.mean(list(metrics['per_class_pq'].values())))\n",
    "    metrics['overall_sq'] = float(np.mean(list(metrics['per_class_sq'].values())))\n",
    "    metrics['overall_rq'] = float(np.mean(list(metrics['per_class_rq'].values())))\n",
    "    metric_results[name] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = {k.replace('_pq', ''):v for k,v in metric_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_vs_dino_metrics = {k.replace('_pq', ''):v for k,v in metric_results.items() if 'ADE' in k or 'target' in k or 'experiment_2_full' in k}\n",
    "coco_vs_dino_instance_counts = {k:v for k,v in instance_counts.items() if 'ADE' in k or 'target' in k or 'experiment_2_full' in k}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_name(idx):\n",
    "    return f'val_{str(idx).zfill(4)}_input'\n",
    "\n",
    "def get_prediction_name(idx, model_name):\n",
    "    return f'val_{str(idx).zfill(4)}_pred_{model_name}'\n",
    "\n",
    "def get_target_name(idx):\n",
    "    return f'val_{str(idx).zfill(4)}_target'\n",
    "\n",
    "def get_image(idx):\n",
    "    img = Image.open(f'../output/{get_image_name(idx)}.png')\n",
    "    return img\n",
    "\n",
    "def get_prediction(idx, model_name):\n",
    "    img = Image.open(f'../output/{get_prediction_name(idx, model_name)}.png')\n",
    "    return img\n",
    "\n",
    "def get_target(idx):\n",
    "    img = Image.open(f'../output/{get_target_name(idx)}.png')\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def get_image_metrics(idx, model_name):\n",
    "    metrics = {}\n",
    "    metrics['pq'] = metric_results[get_prediction_name(idx, model_name)]['overall_pq']\n",
    "    metrics['sq'] = metric_results[get_prediction_name(idx, model_name)]['overall_sq']\n",
    "    metrics['rq'] = metric_results[get_prediction_name(idx, model_name)]['overall_rq']\n",
    "    metrics['per_class_pq'] = metric_results[get_prediction_name(idx, model_name)]['per_class_pq']\n",
    "    metrics['per_class_sq'] = metric_results[get_prediction_name(idx, model_name)]['per_class_sq']\n",
    "    metrics['per_class_rq'] = metric_results[get_prediction_name(idx, model_name)]['per_class_rq']\n",
    "    return metrics\n",
    "\n",
    "def get_image_instance_count(idx, class_name):\n",
    "    return instance_counts[get_target_name(idx)][class_name]\n",
    "    \n",
    "def get_images_by_indexes(indexes, model_names):\n",
    "    for idx in indexes:\n",
    "        get_image(idx)\n",
    "        get_target(idx)\n",
    "\n",
    "\n",
    "        combined_data = {}\n",
    "        for model_name in model_names:\n",
    "            ade_data =coco_vs_dino_metrics[get_prediction_name(idx, model_name)][\"per_class_pq\"]\n",
    "            combined_data[model_name] = ade_data\n",
    "            # Create a pandas DataFrame\n",
    "        df = pd.DataFrame(combined_data)\n",
    "\n",
    "        # Ensure all keys from both dictionaries are included as rows\n",
    "        all_classes = sorted(list(set(ade_data.keys())))\n",
    "        df = df.reindex(all_classes).fillna(0.0) # Fill missing values with 0.0\n",
    "        df.columns = model_names\n",
    "        # Convert the DataFrame to a Markdown table\n",
    "        markdown_table = df.to_markdown(floatfmt=\".2f\") # Added floatfmt=\".2f\"\n",
    "\n",
    "        print(markdown_table)\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(model_names)+2, figsize=(5*len(model_names)+10, 10))\n",
    "\n",
    "        axes[0].imshow(get_image(idx))\n",
    "        axes[0].set_title(f'Input: {idx}')\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(get_target(idx))\n",
    "        axes[1].set_title('Target')\n",
    "        axes[1].axis('off')\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            axes[i+2].imshow(get_prediction(idx, model_name))\n",
    "            axes[i+2].set_title(model_name)\n",
    "            axes[i+2].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_images_by_class_names(class_names):\n",
    "    instance_counts_target = {k:v for k,v in instance_counts.items() if 'target' in k}\n",
    "    matching_images = []\n",
    "\n",
    "    \n",
    "    if type(class_names) == str:\n",
    "        class_names = [class_names]\n",
    "    # For each image\n",
    "    for img_name, instance_count in instance_counts_target.items():\n",
    "        # Check if all specified class names are present in this image\n",
    "        all_classes_present = all(class_name in instance_count.keys() for class_name in class_names)\n",
    "        \n",
    "        # If all specified classes are in this image, add it to our matching images\n",
    "        if all_classes_present:\n",
    "            # Extract the base image name without the \"_target\" suffix\n",
    "            idx = int(img_name.split('_')[1])\n",
    "            matching_images.append(idx)\n",
    "    \n",
    "    return matching_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. COCO vs DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dino_pq_diffs = []\n",
    "\n",
    "for i in range(2000):\n",
    "    dino_pq = coco_vs_dino_metrics[get_prediction_name(i, 'ADE_DINO_WEIGHTS')]['overall_pq']\n",
    "    coco_pq = coco_vs_dino_metrics[get_prediction_name(i, 'experiment_2_full')]['overall_pq']\n",
    "    coco_dino_pq_diffs.append((i,dino_pq - coco_pq))\n",
    "\n",
    "# Sort the lists by the pq value in descending order and take the top 10\n",
    "top_5_dino_is_better = sorted(coco_dino_pq_diffs, key=lambda x: x[1], reverse=True)[:5]\n",
    "top_5_coco_is_better = sorted(coco_dino_pq_diffs, key=lambda x: x[1], reverse=False)[:5]\n",
    "\n",
    "print(\"Top 20 where DINO is better:\", top_5_dino_is_better)\n",
    "print(\"Top 20 where COCO is better:\", top_5_coco_is_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Most PQ Differences (Image Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Where DINOv2 is superior (dino_pq - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in top_5_dino_is_better:\n",
    "    \n",
    "\n",
    "\n",
    "    idx = pair[0]\n",
    "    \n",
    "    ade_data =coco_vs_dino_metrics[get_prediction_name(idx, \"ADE_DINO_WEIGHTS\")][\"per_class_pq\"]\n",
    "    exp_data =coco_vs_dino_metrics[get_prediction_name(idx, \"experiment_2_full\")][\"per_class_pq\"]\n",
    "\n",
    "    combined_data = {\n",
    "    \"ADE\": ade_data,\n",
    "    \"Experiment 2\": exp_data\n",
    "    }\n",
    "        # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Ensure all keys from both dictionaries are included as rows\n",
    "    all_classes = sorted(list(set(ade_data.keys()) | set(exp_data.keys())))\n",
    "    df = df.reindex(all_classes).fillna(0.0) # Fill missing values with 0.0\n",
    "    df.columns = ['DINOv2', 'COCO']\n",
    "    # Convert the DataFrame to a Markdown table\n",
    "    markdown_table = df.to_markdown(floatfmt=\".2f\") # Added floatfmt=\".2f\"\n",
    "\n",
    "    print(markdown_table)\n",
    "    \n",
    "    dinov2_pq = df['DINOv2'].mean()\n",
    "    coco_pq = df['COCO'].mean()\n",
    "\n",
    "    print(f\"DINOv2 PQ: {dinov2_pq:.2f}, COCO PQ: {coco_pq:.2f}\")\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 10))\n",
    "    axes[0].imshow(get_image(idx))\n",
    "    axes[0].set_title(f'Input: {idx}')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(get_target(idx))\n",
    "    axes[1].set_title('Target')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(get_prediction(idx, 'ADE_DINO_WEIGHTS'))\n",
    "    axes[2].set_title(f'DINOv2 Pretrained Weights')\n",
    "    axes[2].axis('off')\n",
    "    axes[3].imshow(get_prediction(idx, 'experiment_2_full'))\n",
    "    axes[3].set_title(f'COCO Pretrained Weights')\n",
    "    axes[3].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Where COCO is superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in top_5_coco_is_better:\n",
    "    \n",
    "\n",
    "\n",
    "    idx = pair[0]\n",
    "    \n",
    "    ade_data =coco_vs_dino_metrics[get_prediction_name(idx, \"ADE_DINO_WEIGHTS\")][\"per_class_pq\"]\n",
    "    exp_data =coco_vs_dino_metrics[get_prediction_name(idx, \"experiment_2_full\")][\"per_class_pq\"]\n",
    "\n",
    "    combined_data = {\n",
    "    \"ADE\": ade_data,\n",
    "    \"Experiment 2\": exp_data\n",
    "    }\n",
    "        # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Ensure all keys from both dictionaries are included as rows\n",
    "    all_classes = sorted(list(set(ade_data.keys()) | set(exp_data.keys())))\n",
    "    df = df.reindex(all_classes).fillna(0.0) # Fill missing values with 0.0\n",
    "    df.columns = ['DINOv2', 'COCO']\n",
    "    # Convert the DataFrame to a Markdown table\n",
    "    markdown_table = df.to_markdown(floatfmt=\".2f\") # Added floatfmt=\".2f\"\n",
    "\n",
    "    print(markdown_table)\n",
    "    \n",
    "    dinov2_pq = df['DINOv2'].mean()\n",
    "    coco_pq = df['COCO'].mean()\n",
    "\n",
    "    print(f\"DINOv2 PQ: {dinov2_pq:.2f}, COCO PQ: {coco_pq:.2f}\")\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 10))\n",
    "    axes[0].imshow(get_image(idx))\n",
    "    axes[0].set_title(f'Input: {idx}')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(get_target(idx))\n",
    "    axes[1].set_title('Target')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(get_prediction(idx, 'ADE_DINO_WEIGHTS'))\n",
    "    axes[2].set_title(f'DINOv2 Pretrained Weights')\n",
    "    axes[2].axis('off')\n",
    "    axes[3].imshow(get_prediction(idx, 'experiment_2_full'))\n",
    "    axes[3].set_title(f'COCO Pretrained Weights')\n",
    "    axes[3].axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Images By Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DINO vs COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_images_by_class_names(['table',\n",
    "                                  'desk',\n",
    "                                  'coffee table, cocktail table'\n",
    "                                  ]\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "models = ['ADE_DINO_WEIGHTS', 'experiment_2_full']\n",
    "\n",
    "get_images_by_indexes(idxs, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with most instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_total_instance={}\n",
    "for k, v in instance_counts.items():\n",
    "    if 'target' in k:\n",
    "        img_total_instance[k] = sum(v.values())\n",
    "\n",
    "sorted_img_total_instance = sorted(img_total_instance.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_img_total_instance = [int(x[0].split('_')[1]) for x in sorted_img_total_instance]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_images_by_indexes(sorted_img_total_instance[:10], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EoMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
