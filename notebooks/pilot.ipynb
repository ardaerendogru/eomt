{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied LoRA to the backbone.\n",
      "trainable params: 2,621,440 || all params: 312,328,192 || trainable%: 0.8393\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Â© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: tuple[int, int],\n",
    "        patch_size=16,\n",
    "        backbone_name=\"vit_large_patch14_reg4_dinov2\",\n",
    "        use_lora: bool = True,\n",
    "        lora_r: int = 32,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.1,\n",
    "        lora_target_modules: list[str] | None = None,\n",
    "        num_lora_free_blocks: int = 4,  # Number of final blocks to exclude from LoRA\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=0,\n",
    "        )\n",
    "\n",
    "        self.use_lora = use_lora\n",
    "        if self.use_lora:\n",
    "            if lora_target_modules is None:\n",
    "                lora_target_modules = [\"qkv\"]\n",
    "\n",
    "            # Determine the specific target modules for LoRA based on blocks\n",
    "            total_blocks = len(self.backbone.blocks)\n",
    "            num_lora_blocks = total_blocks - num_lora_free_blocks\n",
    "            if num_lora_blocks < 0:\n",
    "                 raise ValueError(f\"num_lora_free_blocks ({num_lora_free_blocks}) cannot be greater than total blocks ({total_blocks})\")\n",
    "\n",
    "            specific_lora_target_modules = []\n",
    "            for i in range(num_lora_blocks):\n",
    "                for module_name in lora_target_modules:\n",
    "                    # Assuming standard timm naming convention: blocks.{idx}.attn.{module}\n",
    "                    # Adjust if backbone structure differs\n",
    "                    specific_lora_target_modules.append(f\"blocks.{i}.attn.{module_name}\")\n",
    "            \n",
    "            # # Freeze parameters in the LoRA blocks before applying PEFT\n",
    "            # for i in range(num_lora_blocks):\n",
    "            #     for param in self.backbone.blocks[i].parameters():\n",
    "            #         param.requires_grad = False\n",
    "\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                target_modules=specific_lora_target_modules, # Use the specific list\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                modules_to_save=[],\n",
    "            )\n",
    "            self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "            print(\"Applied LoRA to the backbone.\")\n",
    "            self.backbone.print_trainable_parameters()\n",
    "\n",
    "        pixel_mean = torch.tensor(self.backbone.default_cfg[\"mean\"]).reshape(\n",
    "            1, -1, 1, 1\n",
    "        )\n",
    "        pixel_std = torch.tensor(self.backbone.default_cfg[\"std\"]).reshape(1, -1, 1, 1)\n",
    "\n",
    "        self.register_buffer(\"pixel_mean\", pixel_mean)\n",
    "        self.register_buffer(\"pixel_std\", pixel_std)\n",
    "\n",
    "vit = ViT(img_size=(1280, 1280), use_lora=True, num_lora_free_blocks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating ViT model (use_lora=True, num_lora_free_blocks=4)...\n",
      "Applied LoRA to the backbone.\n",
      "trainable params: 2,621,440 || all params: 305,975,296 || trainable%: 0.8567\n",
      "------------------------------\n",
      "Checking parameter frozen status:\n",
      "\n",
      "--- Frozen Parameters (343) ---\n",
      "  backbone.base_model.model.cls_token\n",
      "  backbone.base_model.model.reg_token\n",
      "  backbone.base_model.model.pos_embed\n",
      "  backbone.base_model.model.patch_embed.proj.weight\n",
      "  backbone.base_model.model.patch_embed.proj.bias\n",
      "  ...\n",
      "(Total frozen: 343)\n",
      "\n",
      "--- Trainable Parameters (40) ---\n",
      "  backbone.base_model.model.blocks.0.attn.qkv.lora_A.default.weight\n",
      "  backbone.base_model.model.blocks.0.attn.qkv.lora_B.default.weight\n",
      "  backbone.base_model.model.blocks.1.attn.qkv.lora_A.default.weight\n",
      "  backbone.base_model.model.blocks.1.attn.qkv.lora_B.default.weight\n",
      "  backbone.base_model.model.blocks.2.attn.qkv.lora_A.default.weight\n",
      "(Total trainable params: 40, values: 2,621,440)\n",
      "\n",
      "Note: If LoRA is enabled, compare the count with the 'print_trainable_parameters()' output during initialization.\n",
      "------------------------------\n",
      "Total parameters in model instance: 305,975,296\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "# Adjust the relative path '..' if your notebook is nested deeper\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    print(f\"Adding {module_path} to sys.path\")\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Ensure the models module can be found\n",
    "try:\n",
    "    from models.vit import ViT\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing ViT: {e}\")\n",
    "    print(\"Please ensure 'models/vit.py' exists and the correct path is added to sys.path.\")\n",
    "    # Set ViT to None or raise error to prevent proceeding\n",
    "    ViT = None\n",
    "\n",
    "if ViT:\n",
    "    # --- Configuration (Adjust as per your model instantiation) ---\n",
    "    IMG_SIZE = (224, 224)  # Example image size\n",
    "    USE_LORA = True       # Set True to check LoRA effect, False otherwise\n",
    "    NUM_LORA_FREE_BLOCKS = 4 # Match your ViT init (default is 4)\n",
    "    # ---\n",
    "\n",
    "    print(f\"Instantiating ViT model (use_lora={USE_LORA}, num_lora_free_blocks={NUM_LORA_FREE_BLOCKS})...\")\n",
    "    # Instantiate the model (this might print LoRA info)\n",
    "    model = ViT(\n",
    "        img_size=IMG_SIZE,\n",
    "        use_lora=USE_LORA,\n",
    "        num_lora_free_blocks=NUM_LORA_FREE_BLOCKS\n",
    "        # Add other necessary args if your ViT constructor needs them\n",
    "    )\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Check requires_grad status\n",
    "    print(\"Checking parameter frozen status:\")\n",
    "    frozen_params_names = []\n",
    "    trainable_params_names = []\n",
    "    total_param_count = 0\n",
    "    trainable_param_count = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        total_param_count += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params_names.append(name)\n",
    "            trainable_param_count += param.numel()\n",
    "        else:\n",
    "            frozen_params_names.append(name)\n",
    "\n",
    "    print(f\"\\n--- Frozen Parameters ({len(frozen_params_names)}) ---\")\n",
    "    if frozen_params_names:\n",
    "        # Print the first few frozen parameters as examples\n",
    "        for i, name in enumerate(frozen_params_names[:5]):\n",
    "            print(f\"  {name}\")\n",
    "        if len(frozen_params_names) > 100:\n",
    "            print(\"  ...\")\n",
    "        print(f\"(Total frozen: {len(frozen_params_names)})\")\n",
    "    else:\n",
    "        print(\"  None\")\n",
    "\n",
    "    print(f\"\\n--- Trainable Parameters ({len(trainable_params_names)}) ---\")\n",
    "    if trainable_params_names:\n",
    "        # Print the first few trainable parameters as examples\n",
    "        for i, name in enumerate(trainable_params_names[:5]):\n",
    "            print(f\"  {name}\")\n",
    "        if len(trainable_params_names) > 100:\n",
    "            print(\"  ...\")\n",
    "        print(f\"(Total trainable params: {len(trainable_params_names)}, values: {trainable_param_count:,})\")\n",
    "        # If using LoRA, the count should match the output of model.backbone.print_trainable_parameters()\n",
    "        if USE_LORA:\n",
    "             print(\"\\nNote: If LoRA is enabled, compare the count with the 'print_trainable_parameters()' output during initialization.\")\n",
    "    else:\n",
    "        print(\"  None\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total parameters in model instance: {total_param_count:,}\")\n",
    "\n",
    "    # Optional: Clean up memory in notebook\n",
    "    # del model\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (backbone): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): VisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (patch_drop): Identity()\n",
       "        (norm_pre): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (12): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (13): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (14): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (15): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (16): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (17): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (18): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (19): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (20): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (21): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (22): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (23): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc_norm): Identity()\n",
       "        (head_drop): Dropout(p=0.0, inplace=False)\n",
       "        (head): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VisionTransformer' object has no attribute 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Example usage: Analyze QKV weights from layer 5\u001b[39;00m\n\u001b[32m     39\u001b[39m layer_num = \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# Choose the layer you want to analyze\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m q_weights, k_weights, v_weights = \u001b[43mextract_qkv_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m analyze_qkv_pca(q_weights, k_weights, v_weights, layer_num)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# You can loop through multiple layers if needed\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# for layer_num in range(12): # Assuming 12 layers, adjust as needed\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#     q_weights, k_weights, v_weights = extract_qkv_weights(vit, layer_num)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#     analyze_qkv_pca(q_weights, k_weights, v_weights, layer_num)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mextract_qkv_weights\u001b[39m\u001b[34m(model, layer_num)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_qkv_weights\u001b[39m(model, layer_num):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     q_weights = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m.model.blocks[layer_num].attn.qkv.weight[:model.backbone.model.blocks[layer_num].attn.qkv.weight.shape[\u001b[32m0\u001b[39m]//\u001b[32m3\u001b[39m,:]\n\u001b[32m     13\u001b[39m     k_weights = model.backbone.model.blocks[layer_num].attn.qkv.weight[model.backbone.model.blocks[layer_num].attn.qkv.weight.shape[\u001b[32m0\u001b[39m]//\u001b[32m3\u001b[39m:\u001b[32m2\u001b[39m*model.backbone.model.blocks[layer_num].attn.qkv.weight.shape[\u001b[32m0\u001b[39m]//\u001b[32m3\u001b[39m,:]\n\u001b[32m     14\u001b[39m     v_weights = model.backbone.model.blocks[layer_num].attn.qkv.weight[\u001b[32m2\u001b[39m*model.backbone.model.blocks[layer_num].attn.qkv.weight.shape[\u001b[32m0\u001b[39m]//\u001b[32m3\u001b[39m:,:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/EoMT/lib/python3.12/site-packages/torch/nn/modules/module.py:1729\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1728\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'VisionTransformer' object has no attribute 'backbone'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import timm\n",
    "# Assuming 'vit' is your ViT model (e.g., a DINOv2 model)\n",
    "# and it has been properly initialized and (if applicable) loaded with weights.\n",
    "\n",
    "model = timm.create_model(\"vit_large_patch14_reg4_dinov2\", pretrained=True, img_size=(1280, 1280), patch_size=14, num_classes=0)\n",
    "# Function to extract QKV weights from a given layer\n",
    "def extract_qkv_weights(model, layer_num):\n",
    "    # Access blocks directly on the model\n",
    "    qkv_layer = model.blocks[layer_num].attn.qkv\n",
    "    qkv_weight = qkv_layer.weight\n",
    "    \n",
    "    # Calculate the split point based on the weight shape\n",
    "    embed_dim = qkv_weight.shape[1] # Dimension of each Q, K, V head\n",
    "    num_heads = model.blocks[layer_num].attn.num_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "\n",
    "    # The qkv.weight tensor shape is typically (3 * embed_dim, embed_dim) \n",
    "    # or sometimes (embed_dim, 3 * head_dim) depending on implementation.\n",
    "    # Let's assume (3 * embed_dim, ...) as per original slicing logic.\n",
    "    total_dim = qkv_weight.shape[0]\n",
    "    dim_per_type = total_dim // 3\n",
    "\n",
    "    q_weights = qkv_weight[:dim_per_type, :]\n",
    "    k_weights = qkv_weight[dim_per_type:2*dim_per_type, :]\n",
    "    v_weights = qkv_weight[2*dim_per_type:, :]\n",
    "    \n",
    "    return q_weights.detach().cpu().numpy(), k_weights.detach().cpu().numpy(), v_weights.detach().cpu().numpy()\n",
    "\n",
    "# Function to apply PCA and visualize explained variance\n",
    "def analyze_qkv_pca(q_weights, k_weights, v_weights, layer_num):\n",
    "    # Combine Q, K, V weights for PCA\n",
    "    combined_weights = np.concatenate([q_weights, k_weights, v_weights], axis=0)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(combined_weights)\n",
    "\n",
    "    # Plot explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_variance)\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    plt.title(f\"PCA Explained Variance - Layer {layer_num}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Analyze QKV weights from layer 5\n",
    "layer_num = 5  # Choose the layer you want to analyze\n",
    "q_weights, k_weights, v_weights = extract_qkv_weights(model, layer_num)\n",
    "analyze_qkv_pca(q_weights, k_weights, v_weights, layer_num)\n",
    "\n",
    "# You can loop through multiple layers if needed\n",
    "# for layer_num in range(12): # Assuming 12 layers, adjust as needed\n",
    "#     q_weights, k_weights, v_weights = extract_qkv_weights(vit, layer_num)\n",
    "#     analyze_qkv_pca(q_weights, k_weights, v_weights, layer_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/arda/miniconda3/envs/EoMT/lib/python3.12/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/arda/miniconda3/envs/EoMT/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EoMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
